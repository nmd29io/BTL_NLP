# HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng Nhanh

## CÃ i Äáº·t

```bash
cd BTL_NLP
pip install -r requirements.txt
```

## Cháº¡y ToÃ n Bá»™ Pipeline

### CÃ¡ch 1: Sá»­ dá»¥ng main.py (Khuyáº¿n nghá»‹)

```bash
# Cháº¡y táº¥t cáº£: xá»­ lÃ½ dá»¯ liá»‡u -> huáº¥n luyá»‡n -> Ä‘Ã¡nh giÃ¡ -> bÃ¡o cÃ¡o
python main.py --mode all

# Hoáº·c cháº¡y tá»«ng bÆ°á»›c:
python main.py --mode data      # Chá»‰ xá»­ lÃ½ dá»¯ liá»‡u
python main.py --mode train     # Chá»‰ huáº¥n luyá»‡n
python main.py --mode eval      # Chá»‰ Ä‘Ã¡nh giÃ¡
```

### CÃ¡ch 2: Cháº¡y tá»«ng script riÃªng

```bash
# 1. Xá»­ lÃ½ dá»¯ liá»‡u
python data_processing.py

# 2. Huáº¥n luyá»‡n mÃ´ hÃ¬nh
python train.py

# 3. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
python evaluate.py

# 4. Táº¡o bÃ¡o cÃ¡o
python report.py
```

## ğŸ”„ Quy TrÃ¬nh LÃ m Viá»‡c TiÃªu Chuáº©n

### 1. TrÃªn MÃ¡y CÃ¡ NhÃ¢n (Local) - Code & Test Nháº¹
- **Má»¥c Ä‘Ã­ch**: Viáº¿t code, sá»­a lá»—i, cháº¡y thá»­ vá»›i dá»¯ liá»‡u nhá».
- **Thao tÃ¡c**:
    1. Sá»­a code trÃªn mÃ¡y.
    2. Cháº¡y thá»­: `python main.py --mode data` (Ä‘á»ƒ cháº¯c cháº¯n khÃ´ng lá»—i).
    3. **Äá»“ng bá»™ lÃªn Git**:
       - Command Prompt: `git_sync.bat`
       - PowerShell: `.\git_sync.bat`

### 2. TrÃªn Google Colab - Train "Háº¡ng Náº·ng"
- **Má»¥c Ä‘Ã­ch**: Táº­n dá»¥ng GPU miá»…n phÃ­ Ä‘á»ƒ train mÃ´ hÃ¬nh lÃ¢u (1 giá»+).
- **Thao tÃ¡c**:
    1. Má»Ÿ notebook `Colab_Run.ipynb`.
    2. Cháº¡y cell Ä‘áº§u tiÃªn Ä‘á»ƒ **tá»± Ä‘á»™ng clone/update code má»›i nháº¥t** tá»« Git.
    3. Cháº¡y cell Train (Ä‘Ã£ set sáºµn 1 giá» hoáº·c hÆ¡n).
    4. **LÆ°u káº¿t quáº£**: Cell cuá»‘i cÃ¹ng sáº½ lÆ°u Model/Result vÃ o Google Drive.

### 3. TrÃªn Kaggle (Alternative) - Khi háº¿t GPU Colab
- **Má»¥c Ä‘Ã­ch**: Thay tháº¿ Colab khi bá»‹ giá»›i háº¡n Usage.
- **Thao tÃ¡c**:
    1. Táº¡o Notebook má»›i trÃªn Kaggle.
    2. Import file `Kaggle_Run.ipynb`.
    3. Báº­t **Internet On** trong Settings.
    4. Cháº¡y toÃ n bá»™ (Run All).
    5. VÃ o tab **Output** Ä‘á»ƒ táº£i file `experiment_results.zip`.

---

## Demo Nhanh

Sau khi huáº¥n luyá»‡n xong, báº¡n cÃ³ thá»ƒ test mÃ´ hÃ¬nh:

```bash
python demo.py --sentences "xin chÃ o" "tÃ´i lÃ  sinh viÃªn" "hÃ´m nay trá»i Ä‘áº¹p"
```

## TÃ¹y Chá»‰nh Hyperparameters

### Training vá»›i cáº¥u hÃ¬nh tÃ¹y chá»‰nh:

```bash
python train.py \
    --d_model 512 \
    --n_heads 8 \
    --n_encoder_layers 6 \
    --n_decoder_layers 6 \
    --batch_size 32 \
    --num_epochs 50 \
    --learning_rate 1e-4
```

### Evaluation vá»›i beam size khÃ¡c:

```bash
python evaluate.py \
    --model_path models/best_model.pt \
    --beam_size 10
```

## Cáº¥u TrÃºc ThÆ° Má»¥c Sau Khi Cháº¡y

```
BTL_NLP/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ src_vocab.pkl          # Vocabulary tiáº¿ng Viá»‡t
â”‚   â””â”€â”€ tgt_vocab.pkl          # Vocabulary tiáº¿ng Anh
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pt          # MÃ´ hÃ¬nh tá»‘t nháº¥t
â”‚   â””â”€â”€ checkpoint_epoch_*.pt  # Checkpoints
â””â”€â”€ results/
    â”œâ”€â”€ training_history.png   # Äá»“ thá»‹ loss/perplexity
    â”œâ”€â”€ training_history.pkl   # Dá»¯ liá»‡u training
    â”œâ”€â”€ evaluation_report.txt  # BÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡
    â”œâ”€â”€ beam_predictions.txt   # Káº¿t quáº£ beam search
    â”œâ”€â”€ greedy_predictions.txt  # Káº¿t quáº£ greedy search
    â”œâ”€â”€ references.txt         # References
    â””â”€â”€ final_report.md        # BÃ¡o cÃ¡o tá»•ng há»£p
```

## LÆ°u Ã

1. **Dá»¯ liá»‡u**: Script sáº½ tá»± Ä‘á»™ng táº£i IWSLT dataset. Náº¿u khÃ´ng táº£i Ä‘Æ°á»£c, sáº½ sá»­ dá»¥ng dá»¯ liá»‡u máº«u Ä‘á»ƒ demo.

2. **GPU**: Náº¿u cÃ³ GPU, mÃ´ hÃ¬nh sáº½ tá»± Ä‘á»™ng sá»­ dá»¥ng GPU Ä‘á»ƒ tÄƒng tá»‘c.

3. **Thá»i gian huáº¥n luyá»‡n**: 
   - Vá»›i CPU: ~vÃ i giá» cho 50 epochs
   - Vá»›i GPU: ~30-60 phÃºt cho 50 epochs

4. **Bá»™ nhá»›**: 
   - Dá»¯ liá»‡u nhá»: ~2-4GB RAM
   - Dá»¯ liá»‡u lá»›n: CÃ³ thá»ƒ cáº§n 8GB+ RAM

## Troubleshooting

### Lá»—i: "CUDA out of memory"
- Giáº£m `batch_size` (vÃ­ dá»¥: `--batch_size 16`)
- Giáº£m `max_len` (vÃ­ dá»¥: `--max_len 64`)
- Giáº£m `d_model` hoáº·c `d_ff`

### Lá»—i: "Module not found"
- Cháº¡y: `pip install -r requirements.txt`

### Lá»—i khi táº£i dá»¯ liá»‡u
- Kiá»ƒm tra káº¿t ná»‘i internet
- Script sáº½ tá»± Ä‘á»™ng fallback sang dá»¯ liá»‡u máº«u

## VÃ­ Dá»¥ Sá»­ Dá»¥ng

### Huáº¥n luyá»‡n mÃ´ hÃ¬nh nhá» (Tá»‘i Æ°u cho Demo/Colab - 1 Giá»):
```bash
python train.py \
    --max_time_hours 1.0 \
    --d_model 256 \
    --n_heads 4 \
    --n_encoder_layers 3 \
    --n_decoder_layers 3 \
    --d_ff 1024 \
    --batch_size 128 \
    --max_len 64 \
    --use_amp
```

### Huáº¥n luyá»‡n mÃ´ hÃ¬nh lá»›n (Cháº¥t lÆ°á»£ng cao):
```bash
python train.py \
    --d_model 512 \
    --n_heads 8 \
    --n_encoder_layers 6 \
    --n_decoder_layers 6 \
    --d_ff 2048 \
    --batch_size 32 \
    --num_epochs 100 \
    --learning_rate 1e-4
```

