# Transformer Seq2Seq Machine Translation

Dá»± Ã¡n xÃ¢y dá»±ng mÃ´ hÃ¬nh dá»‹ch mÃ¡y Seq2Seq sá»­ dá»¥ng kiáº¿n trÃºc Transformer tá»« Ä‘áº§u cho bÃ i toÃ¡n dá»‹ch Viá»‡t-Anh.

## ğŸ“‹ Má»¥c Lá»¥c

- [Tá»•ng Quan](#tá»•ng-quan)
- [Cáº¥u TrÃºc Dá»± Ãn](#cáº¥u-trÃºc-dá»±-Ã¡n)
- [CÃ i Äáº·t](#cÃ i-Ä‘áº·t)
- [Sá»­ Dá»¥ng](#sá»­-dá»¥ng)
- [Kiáº¿n TrÃºc](#kiáº¿n-trÃºc)
- [Káº¿t Quáº£](#káº¿t-quáº£)
- [TÃ i Liá»‡u](#tÃ i-liá»‡u)

## ğŸ¯ Tá»•ng Quan

Dá»± Ã¡n nÃ y triá»ƒn khai Ä‘áº§y Ä‘á»§ kiáº¿n trÃºc Transformer tá»« cÃ¡c thÃ nh pháº§n cÆ¡ báº£n nháº¥t:
- âœ… Scaled Dot-Product Attention & Multi-Head Attention
- âœ… Positional Encoding (Sinusoidal)
- âœ… Transformer Encoder & Decoder Layers
- âœ… Training vá»›i Warmup Learning Rate Scheduler
- âœ… Beam Search & Greedy Search Decoding
- âœ… Evaluation vá»›i BLEU Score

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
BTL_NLP/
â”œâ”€â”€ data/                    # ThÆ° má»¥c chá»©a dá»¯ liá»‡u vÃ  vocabulary
â”œâ”€â”€ models/                  # LÆ°u mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
â”œâ”€â”€ results/                 # Káº¿t quáº£, Ä‘á»“ thá»‹ vÃ  bÃ¡o cÃ¡o
â”œâ”€â”€ data_processing.py       # Xá»­ lÃ½ dá»¯ liá»‡u (tokenization, vocabulary, dataloader)
â”œâ”€â”€ transformer.py           # Kiáº¿n trÃºc Transformer tá»« Ä‘áº§u
â”œâ”€â”€ train.py                 # Script huáº¥n luyá»‡n
â”œâ”€â”€ evaluate.py              # Script Ä‘Ã¡nh giÃ¡ vá»›i Beam Search
â”œâ”€â”€ utils.py                 # CÃ¡c hÃ m tiá»‡n Ã­ch (scheduler, visualization)
â”œâ”€â”€ report.py                # Táº¡o bÃ¡o cÃ¡o káº¿t quáº£
â”œâ”€â”€ demo.py                  # Demo dá»‹ch cÃ¢u
â”œâ”€â”€ main.py                  # Script cháº¡y toÃ n bá»™ pipeline
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ README.md                # File nÃ y
â”œâ”€â”€ QUICKSTART.md            # HÆ°á»›ng dáº«n sá»­ dá»¥ng nhanh
â””â”€â”€ ARCHITECTURE.md          # TÃ i liá»‡u ká»¹ thuáº­t chi tiáº¿t
```

## ğŸ”§ CÃ i Äáº·t

```bash
# Clone hoáº·c táº£i dá»± Ã¡n
cd BTL_NLP

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

## ğŸš€ Sá»­ Dá»¥ng

### CÃ¡ch 1: Cháº¡y toÃ n bá»™ pipeline (Khuyáº¿n nghá»‹)

```bash
python main.py --mode all
```

### CÃ¡ch 2: Cháº¡y tá»«ng bÆ°á»›c

```bash
# 1. Xá»­ lÃ½ dá»¯ liá»‡u
python data_processing.py

# 2. Huáº¥n luyá»‡n mÃ´ hÃ¬nh
python train.py

# 3. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
python evaluate.py

# 4. Táº¡o bÃ¡o cÃ¡o
python report.py

# 5. Demo dá»‹ch cÃ¢u
python demo.py --sentences "xin chÃ o" "tÃ´i lÃ  sinh viÃªn"
```

Xem thÃªm chi tiáº¿t trong [QUICKSTART.md](QUICKSTART.md)

## ğŸ—ï¸ Kiáº¿n TrÃºc

### CÃ¡c ThÃ nh Pháº§n ChÃ­nh

1. **Scaled Dot-Product Attention**
   - TÃ­nh toÃ¡n attention scores vá»›i scaling factor âˆšd_k
   - CÃ´ng thá»©c: `Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V`

2. **Multi-Head Attention**
   - Sá»­ dá»¥ng nhiá»u attention heads (máº·c Ä‘á»‹nh: 8 heads)
   - Má»—i head há»c cÃ¡c loáº¡i quan há»‡ khÃ¡c nhau

3. **Positional Encoding (Sinusoidal)**
   - MÃ£ hÃ³a vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i vÃ  tÆ°Æ¡ng Ä‘á»‘i
   - KhÃ´ng cáº§n há»c (fixed encoding)

4. **Transformer Encoder Layer**
   - Multi-Head Self-Attention
   - Feed-Forward Network
   - Residual Connections & Layer Normalization

5. **Transformer Decoder Layer**
   - Masked Multi-Head Self-Attention
   - Multi-Head Cross-Attention (Encoder-Decoder)
   - Feed-Forward Network
   - Residual Connections & Layer Normalization

6. **Decoding**
   - Beam Search (Æ°u tiÃªn)
   - Greedy Search

Xem chi tiáº¿t ká»¹ thuáº­t trong [ARCHITECTURE.md](ARCHITECTURE.md)

## ğŸ“Š Káº¿t Quáº£

Sau khi huáº¥n luyá»‡n, káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u táº¡i thÆ° má»¥c `results/`:

- `training_history.png`: Äá»“ thá»‹ Loss vÃ  Perplexity
- `evaluation_report.txt`: BLEU Score vÃ  metrics
- `final_report.md`: BÃ¡o cÃ¡o tá»•ng há»£p
- `beam_predictions.txt`: Káº¿t quáº£ dá»‹ch vá»›i Beam Search
- `greedy_predictions.txt`: Káº¿t quáº£ dá»‹ch vá»›i Greedy Search

## ğŸ“š TÃ i Liá»‡u

- [QUICKSTART.md](QUICKSTART.md) - HÆ°á»›ng dáº«n sá»­ dá»¥ng nhanh
- [ARCHITECTURE.md](ARCHITECTURE.md) - TÃ i liá»‡u ká»¹ thuáº­t chi tiáº¿t
- [Paper gá»‘c](https://arxiv.org/abs/1706.03762) - "Attention Is All You Need"

## âš™ï¸ Hyperparameters Máº·c Äá»‹nh

```python
d_model = 512
n_heads = 8
n_encoder_layers = 6
n_decoder_layers = 6
d_ff = 2048
dropout = 0.1
batch_size = 32
learning_rate = 1e-4
warmup_steps = 4000
beam_size = 5
```

## ğŸ“ CÃ¡c Ká»¹ Thuáº­t Cáº£i Tiáº¿n

1. **Warmup Learning Rate Scheduler**: TÄƒng dáº§n LR trong giai Ä‘oáº¡n Ä‘áº§u
2. **Gradient Clipping**: NgÄƒn gradient explosion
3. **Beam Search Decoding**: TÃ¬m kiáº¿m tá»‘t hÆ¡n greedy search
4. **Early Stopping**: Dá»«ng sá»›m khi validation loss khÃ´ng cáº£i thiá»‡n
5. **Layer Normalization**: GiÃºp training á»•n Ä‘á»‹nh hÆ¡n

## ğŸ“ Ghi ChÃº

- Dá»± Ã¡n Ä‘Æ°á»£c xÃ¢y dá»±ng hoÃ n toÃ n tá»« Ä‘áº§u, khÃ´ng sá»­ dá»¥ng pre-built Transformer tá»« thÆ° viá»‡n
- Táº¥t cáº£ cÃ¡c thÃ nh pháº§n (Attention, Positional Encoding, Encoder, Decoder) Ä‘á»u Ä‘Æ°á»£c implement tá»« Ä‘áº§u
- Code Ä‘Æ°á»£c comment chi tiáº¿t báº±ng tiáº¿ng Viá»‡t Ä‘á»ƒ dá»… hiá»ƒu

## ğŸ‘¥ ÄÃ³ng GÃ³p

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  nghiÃªn cá»©u.

