# Transformer Seq2Seq Machine Translation

Dá»± Ã¡n xÃ¢y dá»±ng mÃ´ hÃ¬nh dá»‹ch mÃ¡y Seq2Seq sá»­ dá»¥ng kiáº¿n trÃºc Transformer tá»« Ä‘áº§u cho bÃ i toÃ¡n dá»‹ch Viá»‡t-Anh.

## ğŸ“‹ Má»¥c Lá»¥c

- [Tá»•ng Quan](#tá»•ng-quan)
- [Cáº¥u TrÃºc Dá»± Ãn](#cáº¥u-trÃºc-dá»±-Ã¡n)
- [CÃ i Äáº·t](#cÃ i-Ä‘áº·t)
- [Sá»­ Dá»¥ng](#sá»­-dá»¥ng)
- [Kiáº¿n TrÃºc](#kiáº¿n-trÃºc)
- [Káº¿t Quáº£](#káº¿t-quáº£)
- [TÃ i Liá»‡u](#tÃ i-liá»‡u)

## ğŸ¯ Tá»•ng Quan

Dá»± Ã¡n nÃ y triá»ƒn khai Ä‘áº§y Ä‘á»§ kiáº¿n trÃºc Transformer tá»« cÃ¡c thÃ nh pháº§n cÆ¡ báº£n nháº¥t:
- âœ… Scaled Dot-Product Attention & Multi-Head Attention
- âœ… Positional Encoding (Sinusoidal)
- âœ… Transformer Encoder & Decoder Layers
- âœ… Training vá»›i Warmup Learning Rate Scheduler
- âœ… Beam Search & Greedy Search Decoding
- âœ… Evaluation vá»›i BLEU Score

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```mermaid
graph TD
    A["BTL_NLP"] --> B["Source Code"]
    A --> C["Config & Run Scripts"]
    A --> D["Data & Output"]

    B --> B1["transformer.py<br/>(Kiáº¿n trÃºc Model)"]
    B --> B2["train.py<br/>(Train 1h tá»‘i Æ°u)"]
    B --> B3["evaluate.py<br/>(ÄÃ¡nh giÃ¡ BLEU)"]
    B --> B4["data_processing.py<br/>(Xá»­ lÃ½ dá»¯ liá»‡u)"]
    B --> B5["utils.py<br/>(Tiá»‡n Ã­ch)"]

    C --> C1["Colab_Run.ipynb<br/>(Notebook cháº¡y Colab)"]
    C --> C2["git_sync.bat<br/>(Sync code vá»›i GitHub)"]
    C --> C3["requirements.txt"]

    D --> D1["data/<br/>(Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½)"]
    D --> D2["models/<br/>(LÆ°u checkpoint)"]
    D --> D3["results/<br/>(Biá»ƒu Ä‘á»“ & Log)"]
```

### ğŸ“ Chi Tiáº¿t Chá»©c NÄƒng

**1. MÃ£ Nguá»“n Cá»‘t LÃµi (Core Source):**
*   **`transformer.py`**: Chá»©a toÃ n bá»™ kiáº¿n trÃºc Transformer (Multi-Head Attention, Encoder, Decoder, Positional Encoding) Ä‘Æ°á»£c code tá»« Ä‘áº§u (from scratch).
*   **`data_processing.py`**: Phá»¥ trÃ¡ch táº£i dataset IWSLT (Anh-Viá»‡t), xÃ¢y dá»±ng bá»™ tá»« Ä‘iá»ƒn (Vocabulary), vÃ  táº¡o DataLoader.
*   **`train.py`**: Script huáº¥n luyá»‡n chÃ­nh tá»‘i Æ°u cho demo/bÃ i táº­p lá»›n (giá»›i háº¡n 1h, dÃ¹ng Mixed Precision).
*   **`evaluate.py`**: ÄÃ¡nh giÃ¡ model vá»›i BLEU score trÃªn táº­p test.
*   **`utils.py`**: CÃ¡c hÃ m phá»¥ trá»£ (check GPU, váº½ biá»ƒu Ä‘á»“ training, lÆ°u/táº£i checkpoint).

**2. MÃ´i TrÆ°á»ng & Cháº¡y (Environment):**
*   **`Colab_Run.ipynb`**: Notebook cháº¡y toÃ n bá»™ dá»± Ã¡n trÃªn Google Colab.
*   **`requirements.txt`**: Danh sÃ¡ch thÆ° viá»‡n cáº§n thiáº¿t.

**3. Dá»¯ Liá»‡u & Káº¿t Quáº£:**
*   **`data/`**: Chá»©a dá»¯ liá»‡u tokenized.
*   **`models/`**: LÆ°u `best_model.pt` vÃ  `final_model.pt`.
*   **`results/`**: LÆ°u biá»ƒu Ä‘á»“ loss/perplexity vÃ  káº¿t quáº£ dá»‹ch máº«u.

## ğŸ”§ CÃ i Äáº·t

```bash
# Clone hoáº·c táº£i dá»± Ã¡n
cd BTL_NLP

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

## ğŸš€ Sá»­ Dá»¥ng

### CÃ¡ch 1: Cháº¡y toÃ n bá»™ pipeline (Khuyáº¿n nghá»‹)

```bash
python main.py --mode all
```

### CÃ¡ch 2: Cháº¡y tá»«ng bÆ°á»›c

```bash
# 1. Xá»­ lÃ½ dá»¯ liá»‡u
python data_processing.py

# 2. Huáº¥n luyá»‡n mÃ´ hÃ¬nh
python train.py

# 3. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
python evaluate.py

# 4. Táº¡o bÃ¡o cÃ¡o
python report.py

# 5. Demo dá»‹ch cÃ¢u
python demo.py --sentences "xin chÃ o" "tÃ´i lÃ  sinh viÃªn"
```

### CÃ¡ch 3: Cháº¡y trÃªn Google Colab

1. Táº£i file `Colab_Run.ipynb` lÃªn Google Colab.
2. Cháº¡y cÃ¡c cell theo thá»© tá»± Ä‘á»ƒ:
   - Clone repo & cÃ i dependencies.
   - Train model (tá»‘i Æ°u 1 giá») + LÆ°u káº¿t quáº£ vÃ o Drive.
   - Evaluate káº¿t quáº£.

Xem thÃªm chi tiáº¿t trong [QUICKSTART.md](QUICKSTART.md)

## ğŸ—ï¸ Kiáº¿n TrÃºc

### CÃ¡c ThÃ nh Pháº§n ChÃ­nh

1. **Scaled Dot-Product Attention**
   - TÃ­nh toÃ¡n attention scores vá»›i scaling factor âˆšd_k
   - CÃ´ng thá»©c: `Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V`

2. **Multi-Head Attention**
   - Sá»­ dá»¥ng nhiá»u attention heads (máº·c Ä‘á»‹nh: 8 heads)
   - Má»—i head há»c cÃ¡c loáº¡i quan há»‡ khÃ¡c nhau

3. **Positional Encoding (Sinusoidal)**
   - MÃ£ hÃ³a vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i vÃ  tÆ°Æ¡ng Ä‘á»‘i
   - KhÃ´ng cáº§n há»c (fixed encoding)

4. **Transformer Encoder Layer**
   - Multi-Head Self-Attention
   - Feed-Forward Network
   - Residual Connections & Layer Normalization

5. **Transformer Decoder Layer**
   - Masked Multi-Head Self-Attention
   - Multi-Head Cross-Attention (Encoder-Decoder)
   - Feed-Forward Network
   - Residual Connections & Layer Normalization

6. **Decoding**
   - Beam Search (Æ°u tiÃªn)
   - Greedy Search

Xem chi tiáº¿t ká»¹ thuáº­t trong [ARCHITECTURE.md](ARCHITECTURE.md)

## ğŸ“Š Káº¿t Quáº£

Sau khi huáº¥n luyá»‡n, káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u táº¡i thÆ° má»¥c `results/`:

- `training_history.png`: Äá»“ thá»‹ Loss vÃ  Perplexity
- `evaluation_report.txt`: BLEU Score vÃ  metrics
- `final_report.md`: BÃ¡o cÃ¡o tá»•ng há»£p
- `beam_predictions.txt`: Káº¿t quáº£ dá»‹ch vá»›i Beam Search
- `greedy_predictions.txt`: Káº¿t quáº£ dá»‹ch vá»›i Greedy Search

## ğŸ“š TÃ i Liá»‡u

- [QUICKSTART.md](QUICKSTART.md) - HÆ°á»›ng dáº«n sá»­ dá»¥ng nhanh
- [ARCHITECTURE.md](ARCHITECTURE.md) - TÃ i liá»‡u ká»¹ thuáº­t chi tiáº¿t
- [Paper gá»‘c](https://arxiv.org/abs/1706.03762) - "Attention Is All You Need"

## âš™ï¸ Hyperparameters Máº·c Äá»‹nh

```python
d_model = 512
n_heads = 8
n_encoder_layers = 6
n_decoder_layers = 6
d_ff = 2048
dropout = 0.1
batch_size = 32
learning_rate = 1e-4
warmup_steps = 4000
beam_size = 5
```

## ğŸ“ CÃ¡c Ká»¹ Thuáº­t Cáº£i Tiáº¿n

1. **Warmup Learning Rate Scheduler**: TÄƒng dáº§n LR trong giai Ä‘oáº¡n Ä‘áº§u
2. **Gradient Clipping**: NgÄƒn gradient explosion
3. **Beam Search Decoding**: TÃ¬m kiáº¿m tá»‘t hÆ¡n greedy search
4. **Early Stopping**: Dá»«ng sá»›m khi validation loss khÃ´ng cáº£i thiá»‡n
5. **Layer Normalization**: GiÃºp training á»•n Ä‘á»‹nh hÆ¡n

## ğŸ“ Ghi ChÃº

- Dá»± Ã¡n Ä‘Æ°á»£c xÃ¢y dá»±ng hoÃ n toÃ n tá»« Ä‘áº§u, khÃ´ng sá»­ dá»¥ng pre-built Transformer tá»« thÆ° viá»‡n
- Táº¥t cáº£ cÃ¡c thÃ nh pháº§n (Attention, Positional Encoding, Encoder, Decoder) Ä‘á»u Ä‘Æ°á»£c implement tá»« Ä‘áº§u
- Code Ä‘Æ°á»£c comment chi tiáº¿t báº±ng tiáº¿ng Viá»‡t Ä‘á»ƒ dá»… hiá»ƒu

## ğŸ‘¥ ÄÃ³ng GÃ³p

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  nghiÃªn cá»©u.

