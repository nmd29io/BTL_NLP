# -*- coding: utf-8 -*-
"""train_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZjbkC6A0pH3tjiKiC9_hIHO6YETZynza

# Training Transformer với IWSLT trên Google Colab

Notebook này sẽ train mô hình Transformer cho bài toán dịch máy Vi-En trong 1 giờ.

## Bước 1: Cài đặt dependencies
"""

# Cài đặt các thư viện cần thiết
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy matplotlib seaborn tqdm sacrebleu pandas datasets transformers

"""## Bước 2: Upload các file Python cần thiết"""

# Upload các file: transformer.py, data_processing.py, utils.py
from google.colab import files
import os

print("Vui lòng upload các file sau:")
print("1. transformer.py")
print("2. data_processing.py")
print("3. utils.py")
print("\nSử dụng nút Upload bên dưới hoặc chạy:")
print("uploaded = files.upload()")

# Hoặc clone từ GitHub nếu có
# !git clone https://github.com/your-repo/BTL_NLP.git
# os.chdir('BTL_NLP')

# Upload files
uploaded = files.upload()
for fn in uploaded.keys():
    print(f'Đã upload {fn} ({len(uploaded[fn])} bytes)')

"""## Bước 3: Tải dữ liệu IWSLT

## Bước 4: Import các module
"""

# Import các module
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import os
import time

# Import các module tự viết
from data_processing import prepare_data
from transformer import create_transformer_model
from utils import get_device, WarmupScheduler, save_checkpoint, plot_training_history, calculate_perplexity, count_parameters

# Kiểm tra GPU
device = get_device()
print(f"Sử dụng device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDA Version: {torch.version.cuda}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

"""## Bước 5: Cấu hình training"""

# Cấu hình training tối ưu cho 1 giờ
config = {
    'data_dir': 'data',
    'model_dir': 'models',
    'results_dir': 'results',
    'iwslt_dir': 'iwslt_en_vi',

    # Model hyperparameters (nhỏ hơn để train nhanh)
    'd_model': 256,
    'n_heads': 4,
    'n_encoder_layers': 3,
    'n_decoder_layers': 3,
    'd_ff': 1024,
    'dropout': 0.1,

    # Training hyperparameters
    'batch_size': 128,
    'max_len': 64,
    'min_freq': 2,
    'num_epochs': 100,
    'learning_rate': 1e-4,
    'warmup_steps': 2000,
    'clip_grad': 1.0,
    'patience': 10,
    'save_every': 5,
    'weight_decay': 0.0001,

    # Optimization
    'use_amp': True,  # Mixed precision
    'num_workers': 2,  # Colab thường dùng 2 workers
    'max_time_hours': 1.0  # Train trong 1 giờ
}

print("Cấu hình training:")
for key, value in config.items():
    print(f"  {key}: {value}")

"""## Bước 6: Chuẩn bị dữ liệu"""

# Chuẩn bị dữ liệu
print("=" * 60)
print("CHUẨN BỊ DỮ LIỆU")
print("=" * 60)

data = prepare_data(
    data_dir=config['data_dir'],
    max_len=config['max_len'],
    min_freq=config['min_freq'],
    batch_size=config['batch_size'],
    iwslt_dir=config['iwslt_dir'],
    num_workers=config['num_workers']
)

train_loader = data['train_loader']
val_loader = data['val_loader']
src_vocab = data['src_vocab']
tgt_vocab = data['tgt_vocab']

print(f"\n✓ Đã chuẩn bị dữ liệu:")
print(f"  - Train batches: {len(train_loader)}")
print(f"  - Validation batches: {len(val_loader)}")
print(f"  - Source vocab size: {len(src_vocab)}")
print(f"  - Target vocab size: {len(tgt_vocab)}")

"""## Bước 7: Khởi tạo mô hình"""

# Tạo mô hình
print("=" * 60)
print("KHỞI TẠO MÔ HÌNH")
print("=" * 60)

model = create_transformer_model(
    src_vocab_size=len(src_vocab),
    tgt_vocab_size=len(tgt_vocab),
    d_model=config['d_model'],
    n_heads=config['n_heads'],
    n_encoder_layers=config['n_encoder_layers'],
    n_decoder_layers=config['n_decoder_layers'],
    d_ff=config['d_ff'],
    dropout=config['dropout'],
    pad_idx=src_vocab.word2idx[src_vocab.PAD_TOKEN]
)

model = model.to(device)
num_params = count_parameters(model)
print(f"Số lượng tham số: {num_params:,}")

# Loss function
criterion = nn.CrossEntropyLoss(ignore_index=src_vocab.word2idx[src_vocab.PAD_TOKEN])

# Optimizer
optimizer = optim.AdamW(
    model.parameters(),
    lr=config['learning_rate'],
    betas=(0.9, 0.98),
    eps=1e-9,
    weight_decay=config['weight_decay']
)

# Learning rate scheduler
scheduler = WarmupScheduler(
    optimizer,
    d_model=config['d_model'],
    warmup_steps=config['warmup_steps']
)

# Mixed precision scaler
scaler = torch.cuda.amp.GradScaler() if config['use_amp'] and torch.cuda.is_available() else None
if scaler:
    print("✓ Sử dụng Mixed Precision Training (AMP)")

print("✓ Đã khởi tạo mô hình và optimizer")

"""## Bước 8: Training Functions"""

# Copy các hàm training từ train_fast.py
def train_epoch(model, train_loader, criterion, optimizer, scheduler, device,
                clip_grad=1.0, scaler=None, max_time=None, start_time=None):
    """Huấn luyện một epoch với mixed precision"""
    model.train()
    total_loss = 0
    num_batches = 0

    pbar = tqdm(train_loader, desc='Training')
    for batch in pbar:
        # Kiểm tra thời gian
        if max_time and start_time:
            elapsed = time.time() - start_time
            if elapsed >= max_time:
                print(f"\nĐã đạt giới hạn thời gian ({max_time/3600:.2f} giờ)")
                return total_loss / max(num_batches, 1), True

        src = batch['src'].to(device, non_blocking=True)
        tgt_input = batch['tgt_input'].to(device, non_blocking=True)
        tgt_output = batch['tgt_output'].to(device, non_blocking=True)

        optimizer.zero_grad()

        if scaler is not None:
            with torch.cuda.amp.autocast():
                output = model(src, tgt_input)
                output = output.view(-1, output.size(-1))
                tgt_output_flat = tgt_output.view(-1)
                loss = criterion(output, tgt_output_flat)

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
            scaler.step(optimizer)
            scaler.update()
        else:
            output = model(src, tgt_input)
            output = output.view(-1, output.size(-1))
            tgt_output_flat = tgt_output.view(-1)
            loss = criterion(output, tgt_output_flat)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
            optimizer.step()

        scheduler.step()

        total_loss += loss.item()
        num_batches += 1

        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'lr': f'{scheduler.get_lr():.2e}',
            'avg_loss': f'{total_loss/num_batches:.4f}'
        })

    avg_loss = total_loss / num_batches
    return avg_loss, False


def validate(model, val_loader, criterion, device):
    """Đánh giá trên validation set"""
    model.eval()
    total_loss = 0
    num_batches = 0

    with torch.no_grad():
        for batch in tqdm(val_loader, desc='Validation'):
            src = batch['src'].to(device, non_blocking=True)
            tgt_input = batch['tgt_input'].to(device, non_blocking=True)
            tgt_output = batch['tgt_output'].to(device, non_blocking=True)

            output = model(src, tgt_input)
            output = output.view(-1, output.size(-1))
            tgt_output = tgt_output.view(-1)

            loss = criterion(output, tgt_output)
            total_loss += loss.item()
            num_batches += 1

    avg_loss = total_loss / num_batches
    return avg_loss

print("✓ Đã định nghĩa các hàm training")

"""## Bước 9: Bắt đầu Training"""

# Tạo thư mục
os.makedirs(config['model_dir'], exist_ok=True)
os.makedirs(config['results_dir'], exist_ok=True)

# Training history
train_losses = []
val_losses = []
train_perplexities = []
val_perplexities = []
best_val_loss = float('inf')
patience_counter = 0

# Time-based training
max_time_seconds = config['max_time_hours'] * 3600
start_time = time.time()

print("=" * 60)
print("BẮT ĐẦU HUẤN LUYỆN")
print(f"Giới hạn thời gian: {config['max_time_hours']:.2f} giờ")
print("=" * 60)

# Training loop
epoch = 0
should_stop = False

while not should_stop:
    epoch += 1

    # Kiểm tra thời gian
    elapsed = time.time() - start_time
    if elapsed >= max_time_seconds:
        print(f"\nĐã đạt giới hạn thời gian ({config['max_time_hours']:.2f} giờ)")
        break

    print(f"\nEpoch {epoch}")
    print("-" * 60)

    # Train
    remaining_time = max_time_seconds - elapsed
    train_loss, time_stopped = train_epoch(
        model, train_loader, criterion, optimizer, scheduler, device,
        config['clip_grad'], scaler, remaining_time, start_time
    )

    if time_stopped:
        should_stop = True

    train_ppl = calculate_perplexity(train_loss)
    train_losses.append(train_loss)
    train_perplexities.append(train_ppl)

    # Validate
    if not should_stop:
        elapsed = time.time() - start_time
        if elapsed < max_time_seconds * 0.95:
            val_loss = validate(model, val_loader, criterion, device)
            val_ppl = calculate_perplexity(val_loss)
            val_losses.append(val_loss)
            val_perplexities.append(val_ppl)

            print(f"Train Loss: {train_loss:.4f} | Train PPL: {train_ppl:.2f}")
            print(f"Val Loss: {val_loss:.4f} | Val PPL: {val_ppl:.2f}")
            print(f"Learning Rate: {scheduler.get_lr():.2e}")
            print(f"Thời gian đã dùng: {(time.time() - start_time)/3600:.2f} giờ")

            # Lưu best model
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_path = os.path.join(config['model_dir'], 'best_model.pt')
                save_checkpoint(model, optimizer, epoch, val_loss, best_model_path)
            else:
                patience_counter += 1

            # Early stopping
            if patience_counter >= config['patience']:
                print(f"\nEarly stopping tại epoch {epoch}")
                should_stop = True
        else:
            print(f"Train Loss: {train_loss:.4f} | Train PPL: {train_ppl:.2f}")
            print(f"Learning Rate: {scheduler.get_lr():.2e}")
            print(f"Thời gian đã dùng: {(time.time() - start_time)/3600:.2f} giờ")
            final_model_path = os.path.join(config['model_dir'], 'final_model.pt')
            save_checkpoint(model, optimizer, epoch, train_loss, final_model_path)

    # Lưu checkpoint định kỳ
    if epoch % config['save_every'] == 0 and not should_stop:
        checkpoint_path = os.path.join(config['model_dir'], f'checkpoint_epoch_{epoch}.pt')
        save_checkpoint(model, optimizer, epoch,
                      val_losses[-1] if val_losses else train_loss,
                      checkpoint_path)

print("\n" + "=" * 60)
print("HOÀN THÀNH HUẤN LUYỆN")
print("=" * 60)
total_time = (time.time() - start_time) / 3600
print(f"Tổng thời gian: {total_time:.2f} giờ")
print(f"Tổng số epochs: {epoch}")
if val_losses:
    print(f"Best Validation Loss: {best_val_loss:.4f}")
    print(f"Best Validation Perplexity: {calculate_perplexity(best_val_loss):.2f}")
print(f"Final Train Loss: {train_losses[-1]:.4f}")
print(f"Final Train Perplexity: {train_perplexities[-1]:.2f}")

"""## Bước 10: Lưu kết quả và tải về"""

# Vẽ đồ thị training history
if len(train_losses) > 0:
    plot_training_history(
        train_losses,
        val_losses if val_losses else train_losses,
        train_perplexities,
        val_perplexities if val_perplexities else train_perplexities,
        save_path=os.path.join(config['results_dir'], 'training_history.png')
    )
    print("✓ Đã lưu đồ thị training history")

# Tải model về máy
print("\nĐang tạo file zip để tải về...")
!zip -r transformer_model.zip models/ results/ data/ 2>/dev/null || echo "Đã tạo zip"

print("\n✓ Hoàn thành! Bạn có thể tải file transformer_model.zip về máy.")

# Tải model về máy
from google.colab import files
files.download('transformer_model.zip')